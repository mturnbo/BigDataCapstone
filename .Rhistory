install.packages("MASS")
library("MASS")
data(cats)
str(cats)
summary(cats)
with(cats, plot(Bwt, Hwt))
library("tm")
library("qdap")
library("tm")
library("RWeka")
library("qdap")
library("NLP")
library("openNLP")
library("ggplot2")
setwd("../data/final/en_US")
twitter.us <- readLines("en_US.twitter.txt", encoding = "UTF-8", skipNul=TRUE)
twitter.us.size <- file.info("en_US.twitter.txt")$size / 1024 / 1024
twitter.us.length <- length(twitter.us)
twitter.us.words <- sum(sapply(gregexpr("\\S+", twitter.us), length))
news.us <- readLines("en_US.news.txt", encoding = "UTF-8", skipNul=TRUE)
news.us.size <- file.info("en_US.news.txt")$size / 1024 / 1024
news.us.length <- length(news.us)
news.us.words <- sum(sapply(gregexpr("\\S+", news.us), length))
blogs.us <- readLines("en_US.blogs.txt", encoding = "UTF-8", skipNul=TRUE)
blogs.us.size <- file.info("en_US.blogs.txt")$size / 1024 / 1024
blogs.us.length <- length(blogs.us)
blogs.us.words <- sum(sapply(gregexpr("\\S+", blogs.us), length))
setwd("../data/final/en_US")
badwords = readLines("badwords.txt")
twitter.us.sample <- twitter.us[sample(1:length(twitter.us), 100)]
twitter.us.sample <- twitter.us[sample(1:length(twitter.us), 1000)]
twitter.us.sample <- twitter.us[sample(1:length(twitter.us), 10000)]
remove(twitter.us.sample)
sample <- c(sample(twitter.us, 10000), sample(news.us, 10000), sample(blogs.us, 10000))
corpus <- VCorpus(VectorSource(sample))                 # build corpus
corpus <- tm_map(corpus, sent_detect)                   # detect sentences
corpus <- tm_map(corpus, removeNumbers)                 # remove numbers
corpus <- tm_map(corpus, stripWhitespace)               # removing whitespace
corpus <- tm_map(corpus, removePunctuation)             # remove special characters
corpus <- tm_map(corpus, content_transformer(tolower))  # lowercase
corpus <- tm_map(corpus,
content_transformer(function(x)
iconv(x, to="UTF-8", sub="byte")),
mc.cores=2)
corpus <- tm_map(corpus,
content_transformer(function(x)
iconv(x, to="UTF-8", sub="byte")),
mc.cores=2)
corpus <- Corpus(VectorSource(sample))                  # build corpus
corpus <- tm_map(corpus, sent_detect)                   # detect sentences
corpus <- tm_map(corpus, removeNumbers)                 # remove numbers
corpus <- tm_map(corpus, stripWhitespace)               # removing whitespace
corpus <- tm_map(corpus, removePunctuation)             # remove special characters
corpus <- tm_map(corpus, content_transformer(tolower))  # lowercase
corpus <- tm_map(corpus,
content_transformer(function(x)
iconv(x, to="UTF-8", sub="byte")),
mc.cores=2)
corpus <- tm_map(corpus, tolower)  # lowercase
corpus <- tm_map(corpus, tolower, lazy = TRUE)  # lowercase
corpus <- tm_map(corpus, removeWords, badwords)         # bad words
corpus <- tm_map(corpus, tolower, lazy = TRUE)          # lowercase
corpus <- tm_map(corpus, removeWords, badwords, lazy = TRUE)         # bad words
cleanText <- data.frame(text=unlist(sapply(corpus, `[`, "content")), stringsAsFactors=F)
cleanText <- data.frame(text=unlist(sapply(corpus, `[`, "content")), stringsAsFactors=F)
corpus
corpus <- Corpus(VectorSource(sample))                  # build corpus
cleanText <- data.frame(text=unlist(sapply(corpus, `[`, "content")), stringsAsFactors=F)
corpus <- tm_map(corpus, sent_detect)                   # detect sentences
corpus <- tm_map(corpus, removeNumbers)                 # remove numbers
cleanText <- data.frame(text=unlist(sapply(corpus, `[`, "content")), stringsAsFactors=F)
corpus <- Corpus(VectorSource(sample))                  # build corpus
corpus <- tm_map(corpus, removeNumbers)                 # remove numbers
corpus <- tm_map(corpus, stripWhitespace)               # removing whitespace
corpus <- tm_map(corpus, removePunctuation)             # remove special characters
corpus <- tm_map(corpus, tolower, lazy = TRUE)          # lowercase
cleanText <- data.frame(text=unlist(sapply(corpus, `[`, "content")), stringsAsFactors=F)
corpus <- Corpus(VectorSource(sample))                  # build corpus
cleanText <- data.frame(text=unlist(sapply(corpus, `[`, "content")), stringsAsFactors=F)
corpus <- tm_map(corpus, stripWhitespace)               # removing whitespace
cleanText <- data.frame(text=unlist(sapply(corpus, `[`, "content")), stringsAsFactors=F)
corpus <- tm_map(corpus, removeNumbers)                 # remove numbers
cleanText <- data.frame(text=unlist(sapply(corpus, `[`, "content")), stringsAsFactors=F)
corpus <- tm_map(corpus, tolower, lazy = TRUE)          # lowercase
cleanText <- data.frame(text=unlist(sapply(corpus, `[`, "content")), stringsAsFactors=F)
corpus <- Corpus(VectorSource(sample))                  # build corpus
corpus <- tm_map(corpus, sent_detect)
cleanText <- data.frame(text=unlist(sapply(corpus, `[`, "content")), stringsAsFactors=F)
corpus <- Corpus(VectorSource(sample))                  # build corpus
#corpus <- tm_map(corpus, sent_detect)                   # detect sentences
corpus <- tm_map(corpus, removeNumbers)                 # remove numbers
corpus <- tm_map(corpus, stripWhitespace)               # removing whitespace
corpus <- tm_map(corpus, removePunctuation)             # remove special characters
#corpus <- tm_map(corpus, tolower, lazy = TRUE)          # lowercase
corpus <- tm_map(corpus, removeWords, badwords, lazy = TRUE)         # bad words
cleanText <- data.frame(text=unlist(sapply(corpus, `[`, "content")), stringsAsFactors=F)
token1 <- NGramTokenizer(cleanText, Weka_control(min = 1, max = 1))
token2 <- NGramTokenizer(cleanText, Weka_control(min = 2, max = 2, delimiters = " \\r\\n\\t.,;:\"()?!"))
token3 <- NGramTokenizer(cleanText, Weka_control(min = 3, max = 3, delimiters = " \\r\\n\\t.,;:\"()?!"))
token4 <- NGramTokenizer(cleanText, Weka_control(min = 4, max = 4, delimiters = " \\r\\n\\t.,;:\"()?!"))
token4 <- NGramTokenizer(cleanText, Weka_control(min = 4, max = 4, delimiters = " \\r\\n\\t.,;:\"()?!"))
saveRDS(token1, file = "../data/final/en_US/sample_onegrams.rds")
getwd()
setwd("~/Documents/Coursera/Johns Hopkins Data Science Track/10 - Data Science Capstone/Feb2017/tasks")
saveRDS(token1, file = "../data/final/en_US/sample_onegrams.rds")
saveRDS(token2, file = "../data/final/en_US/sample_bigrams.rds")
saveRDS(token3, file = "../data/final/en_US/sample_trigrams.rds")
data1 <- data.frame(table(token1))
data1 <- data1[order(data1$Freq, decreasing=TRUE),]
colnames(data1) <- c("Word", "Frequency")
onegrams <- token1
bigrams <- token2
trigrams <- token3
remove(token1, token2, token3)
dfOneGrams <- data.frame(table(onegrams))
dfOneGrams <- dfOneGrams[order(dfOneGrams$Freq, decreasing=TRUE),]
colnames(dfOneGrams) <- c("Word", "Frequency")
dfBiGrams = data.frame(table(bigrams))
dfBiGrams <- dfBiGrams[order(dfBiGrams$Freq, decreasing=TRUE),]
colnames(dfBiGrams) <- c("Word", "Frequency")
dfTriGrams = data.frame(table(trigrams))
dfTriGrams <- dfTriGrams[order(dfTriGrams$Freq, decreasing=TRUE),]
colnames(dfTriGrams) <- c("Word", "Frequency")
ggplot(head(dfOneGrams, 20), aes(x=Word,y=Frequency)) +
geom_bar(stat="Identity", fill="blue") +
ggtitle("Top 20 One-Gram Terms") +
theme(axis.text.x = element_text(angle=45, hjust=1))
ggplot(head(dfBiGrams, 20), aes(x=Word,y=Frequency)) +
geom_bar(stat="Identity", fill="purple") +
ggtitle("Top 20 Two-Gram Terms") +
theme(axis.text.x = element_text(angle=45, hjust=1))
ggplot(head(dfTriGrams, 20), aes(x=Word,y=Frequency)) +
geom_bar(stat="Identity", fill="orange") +
ggtitle("Top 20 Three-Gram Terms") +
theme(axis.text.x = element_text(angle=45, hjust=1))
wordCoverage <- function (words, coverage)
{
frequency <- 0
requiredFrequency <- coverage * sum(words$Frequency)
for (i in 1:nrow(words)) {
if (frequency >= requiredFrequency) {
return (i)
}
frequency <- frequency + words$Frequency[i]
}
}
pcts <- c(10, 20, 30, 40, 50, 60, 70, 80, 90)
wordPcts <- c()
for (p in pcts) {
wordPcts <- c(wordPcts, wordCoverage(data1, (p/100.0)))
}
qplot(pcts, wordPcts, geom=c("line","point")) + geom_text(aes(label=wordPcts), hjust=1.3, vjust=-0.2) +
scale_x_discrete(breaks=pcts, labels=pcts)
corpus <- Corpus(VectorSource(sample))                  # build corpus
corpus <- tm_map(corpus, removeNumbers)                 # remove numbers
corpus <- tm_map(corpus, stripWhitespace)               # removing whitespace
corpus <- tm_map(corpus, removePunctuation)             # remove special characters
corpus <- tm_map(corpus, tolower, lazy = TRUE)          # lowercase
corpus <- tm_map(corpus, removeWords, badwords, lazy = TRUE)         # bad words
cleanText <- data.frame(text=unlist(sapply(corpus, `[`, "content")), stringsAsFactors = FALSE)
corpus <- Corpus(VectorSource(sample))                  # build corpus
corpus <- tm_map(corpus, removeNumbers)                 # remove numbers
corpus <- tm_map(corpus, stripWhitespace)               # removing whitespace
corpus <- tm_map(corpus, removePunctuation)             # remove special characters
corpus <- tm_map(corpus, PlainTextDocument)
cleanText <- data.frame(text=unlist(sapply(corpus, `[`, "content")), stringsAsFactors = FALSE)
corpus <- tm_map(corpus, removeWords, badwords, lazy = TRUE)
cleanText <- data.frame(text=unlist(sapply(corpus, `[`, "content")), stringsAsFactors = FALSE)
